Magic:
  # Training:
  #   root_path: "/home/000GitHub/MalGuise/saved_models/magic/128-256-384-256"
  #   preprocess_root: "/home/wzy/Attack/dataset/train_and_test/"
  #   max_epochs: 200
  #   train_batch_size: 128
  #   test_batch_size: 128
  #   seed: 19920208
  #   only_test_path: "None"
  #   pretrained_path: "/home/000GitHub/MalGuise/saved_models/magic/128-256-384-256/#auc0.9996_tpr0.9823_fpr0.0014_bacc0.9904_magic.pt"
  Model:
    gnn_type: "GCN"         # "GraphSAGE" / "GCN"
    pool_type: "global_max_pool"  # "global_max_pool" / "global_mean_pool"
    acfg_init_dims: 11
    cfg_filters: "128-256-384-256"
    num_classes: 1
    dropout_rate: 0.5
    use_activation: True
    last_activation: "sigmoid"
    model_path: "/home/000GitHub/MalGuise/saved_models/magic/128-256-384-256/best_magic_model.pt"
    IDA_PATH: "/root/IDA_Pro_v6.4/idaq64"
    SCRIPT_PATH: "/OpenMalAttack/scripts/graph_handle_acfg.py"
    tmp_sample_root: "/OpenMalAttack/dataset/tmp"
  # Optimizer:
  #   name: "Adam"                  # Adam / AdamW
  #   learning_rate: 0.002           # initial learning rate
  #   weight_decay: 1e-5            # initial weight decay         # Annealing applied to learning rate after each epoch
  #   criterion: "BCELoss"

Malconv:
  Training:
    root_path: "/home/000GitHub/MalGuise/saved_models/malconv/2m_128"
    preprocess_root: "/home/wzy/Attack/dataset/train_and_test/"
    max_epochs: 200
    train_batch_size: 100
    test_batch_size: 100
    num_workers: 30
    seed: 19920208
    only_test_path: "None"
    pretrained_path: "None"
  Model:
    input_length: 2000000
    window_size: 500
    num_classes: 1
    num_embeddings: 257 # 0-256，共257
    embedding_dim: 8
    conv_out_channels: 128
    dropout_rate: 0 # 设大了效果不好
    use_activation: True
    last_activation: "sigmoid"
    model_path: "/OpenMalAttack/models/malconv/#auc0.9965_tpr0.9320_fpr0.0033_bacc0.9643_malconv.pt"
  Optimizer:
    name: "Adam"                  # Adam / AdamW
    learning_rate: 0.0007           # initial learning rate
    weight_decay: 1e-5            # initial weight decay         # Annealing applied to learning rate after each epoch
    criterion: "BCELoss"
  Data:
    root: "/home/wzy/get_test_dataset_acfg/hash_list"
    hash_to_path: "/home/wzy/get_test_dataset_acfg/hash_list/all_hash_to_path.txt"

Malgraph:
  # Training:
  #   root_path: "/home/000GitHub/MalGuise/saved_models/malgraph"
  #   data_root: "/home/wzy/get_test_dataset_acfg/extracted_data/0529"
  #   dist_port: 1234
  #   max_epochs: 200
  #   train_batch_size: 128
  #   test_batch_size: 128
  #   num_workers: 1
  #   seed: 19920208
  #   only_test_path: "None"
  #   pretrained_path: "/home/000GitHub/MalGuise/saved_models/malgraph/best_malgraph_model.pt"
  # Optimizer:
  #   name: "Adam"                  # Adam / AdamW
  #   learning_rate: 0.0005           # initial learning rate
  #   weight_decay: 1e-5            # initial weight decay         # Annealing applied to learning rate after each epoch
  #   criterion: "BCELoss"
  Model:
    gnn_type: "GraphSAGE"
    pool_type: "global_max_pool"
    acfg_init_dims: 11
    vocab_path: "/OpenMalAttack/configs/train_external_function_name_vocab.jsonl"
    max_vocab_size: 10000
    cfg_filters: "200-200"
    fcg_filters: "200-200"
    skip_att_heads: 0
    dropout_rate: 0
    ablation_models: "Full"
    model_path: "/OpenMalAttack/models/malgraph/best_malgraph_model.pt"
    IDA_PATH: "/root/IDA_Pro_v6.4/idaq64"
    SCRIPT_PATH: "/OpenMalAttack/scripts/graph_handle_acfg.py"
    tmp_sample_root: "/OpenMalAttack/dataset/tmp"